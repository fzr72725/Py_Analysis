201707:
Python note:
1. List comprehension:
result = [x+10 for x in my_list]

a = list('abcdefghijklmnopqrstuvwxyz')
b = list('phqgiumeaylnofdxjkrcvstzwb')
d = {x:y for x,y in zip(a,b)}

2. File operation:
my_file = open('hello.txt')
t = my_file.read()

# function strip is to remove \n
[l.strip() for l in my_file]

3. word/letter counter:
from collections import Counter
c = Counter(content)
* output is a dictionary

4. Set:
my_set = set(content)
* set is to have a unique element collection

5. Dictionary:
my_dict = dict((key,0) for key in my_set)

6. Other operation:
m += 1

string.lower()

function join is to turn a list into a string connected with specific char('' is no connector)
''.join(list)

print [x for x in range (1,5)]
*[1, 2, 3, 4]

print [i for i in range(8, 0, -1)]
[8, 7, 6, 5, 4, 3, 2, 1]

print range(5,-1,-1)
[5, 4, 3, 2, 1, 0]

7. pandas:
*to plot lists:
import matplotlib.pyplot as plt
%matplotlib inline
plt.plot(x,y)




Algorithm note:
1. Sorting:
* total number of value comparison and position exchange determine the efficiency of a sorting algo.
1) Bubble sort
def bubbleSort(alist):
    for passnum in range(len(alist)-1,0,-1):
        for i in range(passnum):
            if alist[i]>alist[i+1]:
                alist[i],alist[i+1] = alist[i+1],alist[i]
    return alist

2) Selection sort:
* Compare to bubble sort, selection sort reduce the exchange effort, each round, exchange only happen ONCE
* Each round, only the largest element is put to the last position of the list
def selectionSort(my_list):
    for index in range(len(my_list)-1,0,-1):
        largest = 0
        print index
        for idx in range(1,index+1):
            if my_list[idx]>my_list[largest]:
                largest = idx
        my_list[largest],my_list[index] = my_list[index],my_list[largest]
    return my_list
    
3) Insertion sort:
* O(n) = n**2
def insertionSort(my_list):
    for item in range(1,len(my_list)):
        for i in range(item-1,-1,-1):
            if my_list[i] > my_list[item]:
                my_list[i],my_list[item] = my_list[item],my_list[i]
                item = i
    return my_list



Statistics:
1. Permutation:
1) In general, n distinct objects can be arranged in n! ways.
2) The number of permutations of n distinct objects taken r at a time, Pnr=n*(n-1)*(n-2)...*(n-(r-1))
3) The number of different permutations of n objects of which n1 are of one kind, n2 are of a second kind, ... nk are of a k-th kind is n!/(n1!*n2!*...nk!)
4) There are \displaystyle{\left({n}-{1}\right)}!(n−1)! ways to arrange n distinct objects in a circle (where the clockwise and anti-clockwise arrangements are regarded as distinct.)

2. Combination:
1) Cnr=Pnr/r!
2) Cn0=Cnn=1
3) Cnr=Cn(n-r)

3. Probability terms:
1)Experiment: This is any process of observation or procedure that:

-Can be repeated (theoretically) an infinite number of times; and

-Has a well-defined set of possible outcomes.

2)Sample space: This is the set of all possible outcomes of an experiment.

3)Event: This is a subset of the sample space of an experiment.


4. Common probability problems:
1) Lottery odds:
http://www.intmath.com/counting-probability/singapore-toto.php
2) 

5. Conditional probability:
P(A|B): If A and B are two events, the probability that A occurs given that B has occurred
P(A|B)<student is French if he's female> = P(A⋂B)<student is French and female>/P(B)<student is female>

6. Independent and dependent probability:
1) Indepedent:
P(A⋂B) = P(A)*P(B)
2) Dependent:
P(A⋂B) = P(A)*P(B|A) = P(A)*P(B⋂A)/P(A)
* mutually exclusive events are NEVER independent, if A and B are mutually exclusive, P(A⋂B) = 0

7. non mutually exclusive events probability:
P(A or B) = P(A)+P(B)-P(A⋂B)

8. Bayes' Therom:
If E1 and E2 are mutually exclusive:
P(E1∣E)=P(E1⋂E)/[P(E2⋂E)+P(E1⋂E)]

9. If use conditional probability or P(A or B):
See if the given probability is describing seperate events (there are steps), if so, conditional probability, if not, like defective check, it's P(A or B)

10. Steps of solve a problem:
1) define all events
2) determine the relations between events(independent events<IE>/dependent events<DE>/ME/non-ME)
3) use appropriate formula:
IE:
P(A⋂B) = P(A)*P(B)
P(A or B) = P(A)+P(B)-P(A⋂B)

DE:
P(A⋂B) = P(A)*P(B|A) = P(A)*P(B⋂A)/P(A)
P(A or B) = P(A)+P(B)-P(A⋂B)

ME:
P(A⋂B) = 0
P(A or B) = P(A)+P(B)

non_ME:
P(A⋂B) = P(A)*P(B|A) = P(A)*P(B⋂A)/P(A)
P(A or B) = P(A)+P(B)-P(A⋂B)

11. Probability Distributions:
1) Random Variables
P(X) or P(x1), P(x2)...
2) Discrete probability distribution:


3)Probability density function:
* the area under the bell curve between two variables, The area under each curve is \displaystyle{1}1
∫abf(x)dx=P(a≤X≤b)

4)Probabilities As Relative Frequency
* If an experiment is performed a sufficient number of times, then in the long run, the relative frequency of an event is called the probability of that event occurring.

5) Expected Value:
* X represent a discrete random variable
E(X) = μ = Σ (xi × P(xi))
* is also called the mean of the probability distribution

4) Variance of a Random Variable
V(X) = σ2 = Σ[{X − E(X)}2 × P(X) ]

5) Standard Deviation of the Probability Distribution
σ=√V(X)
* describes the spread of the distribution. Small standard deviation means small spread, large standard deviation means large spread

12. The Binomial Probability Distribution
Binomial variable: only two kinds of outcome: true vs. false; yes vs. no, etc
1) the probability of successes in n binomial trials:
http://www.intmath.com/counting-probability/12-binomial-probability-distributions.php

2) Mean and Variance of Binomial Distribution:
E(X) = μ = np
V(X) = σ2 = npq

3) Binomial probability distribution is always finite. You can only have a finite number of x.

13. The Poisson Probability Distribution
http://www.intmath.com/counting-probability/13-poisson-probability-distribution.php
Time of success in two disjoint time intervals are independent
- car accident
- death rate in army due to horse kicking
- birth defect or mutation

E(X) = μ
V(X) = μ

14. Exponential distribution:
P(X) = λ e**(−λx)

15. Uniform Distribution:
P(X) = 1(b-a) if x is in [a,b], otherwise, P(X) = 0

16. Normal distribution:
1) How to calculate probability of normal distribution:
The probability of P(x < a) is: Φ(a)
The probability of P(x > a) is: 1 – Φ(a)
The probability of P(a < Z < b) is: Φ(b) – Φ(a)


2) empirical/three-sigma rule:
for a normal distribution:
68% of the data will fall within one standard deviation of the mean.
95% of the data will fall within two standard deviations of the mean.
Almost all (99.7%) of the data will fall within three standard deviations of the mean

17. Descriptive Statistics
IQR= Q3-Q1
mode is the value of the number which appears the most frequent.

19. Relationship between 2 quantitative variables:
y = a+bx
b = Slope = (N∑XY - (∑X)(∑Y)) / (N∑X**2 - (∑X)**2)
a = mean(Y)-bmean(X)

20. Modeling:
1) Linear Regression:
Predict a dependent continuous vale

2) Logistic Regression:
Predict the probability of a value, bounded between (0,1)
* Changing the $\beta_0$ value shifts the curve horizontally, whereas changing the $\beta_1$ value changes the slope of the curve.
* Positive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability).

3) Cross Validation (CV):
- CV is used to select the best model to train. Here, "two different models" would be: linear regression vs neural network. Once we have used cross-validation to select the better performing model, we train that model (whether it be the linear regression or the neural network) on all the data. 
- k-folder CV: k iterations over the sample data using a proportion for training and CV
- After k-fold cross validation, we’ll get k different model estimation errors (e1, e2 …..ek)(MSE for quantitative responses, misclassification rate for qualtative responses). Then we take standard deviation of all errors. Lower value of standard deviation suggests our model does not vary a lot with different subset of training data.
4) Over-fitting:
Over-fitting is normally caused by high model complexity over the training data.
5) Bias–variance tradeoff:
- Error due to Bias: The error due to bias is taken as the difference between the expected (or average) prediction of our model and the correct value which we are trying to predict.
high bias = underfitting
- Error due to Variance: The error due to variance is taken as the variability of a model prediction for a given data point.(the variance from k-folder CV for example)
high variance = overfitting
6)Interpret coefficient:
Interpreting the "temp" coefficient ($\beta_1$):
It is the change in y divided by change in x, or the "slope".
Thus, a temperature increase of 1 degree Celsius is associated with a rental increase of 9.17 bikes.
This is not a statement of causation.
$\beta_1$ would be negative if an increase in temperature was associated with a decrease in rentals.

7)Ways to measure accuracy of linear regression model:
- MAE: mean of error
- MSE: mean of error square(variance)
- RMSE: mean of standard diviation
- Cross Validation: evaluating the variance/different models


21. Sensitivity and Specificity:
- True positive rate (or sensitivity): TPR=TP/(TP+FN)TPR=TP/(TP+FN)
- False positive rate: FPR=FP/(FP+TN)FPR=FP/(FP+TN)
- True negative rate (or specificity): TNR=TN/(FP+TN)
https://stats.stackexchange.com/questions/61829/relation-between-true-positive-false-positive-false-negative-and-true-negative
- ROC curve plots TP and TN when using different predicting positive threshold 
 
 