201707:
Python note:
1. List comprehension:
result = [x+10 for x in my_list]

a = list('abcdefghijklmnopqrstuvwxyz')
b = list('phqgiumeaylnofdxjkrcvstzwb')
d = {x:y for x,y in zip(a,b)}

2. File operation:
my_file = open('hello.txt')
t = my_file.read()

# function strip is to remove \n
[l.strip() for l in my_file]

3. word/letter counter:
from collections import Counter
c = Counter(content)
* output is a dictionary

4. Set:
my_set = set(content)
* set is to have a unique element collection

5. Dictionary:
my_dict = dict((key,0) for key in my_set)

6. Other operation:
m += 1

string.lower()

function join is to turn a list into a string connected with specific char('' is no connector)
''.join(list)

print [x for x in range (1,5)]
*[1, 2, 3, 4]

print [i for i in range(8, 0, -1)]
[8, 7, 6, 5, 4, 3, 2, 1]

print range(5,-1,-1)
[5, 4, 3, 2, 1, 0]

Using dictionary to count words/letters:
>>> letter_counts = {}
>>> for letter in "Mississippi":
...     letter_counts[letter] = letter_counts.get(letter, 0) + 1 # For each letter in the string, we find the current count 
# (possibly zero) and increment it <dictionary function "get" returns 0 if the key doesn't exist, otherwise returns the value of the key>
...
>>> letter_counts
{'M': 1, 's': 4, 'p': 2, 'i': 4}

To "sort" a dictionary:
letter_items = list(letter_counts.items())
letter_items.sort()

7. pandas:
*to plot lists:
import matplotlib.pyplot as plt
%matplotlib inline
plt.plot(x,y)

8. Algorithms:
* This algorithm cheatsheet is super helpful!
http://bigocheatsheet.com/


Algorithm note:
1. Sorting:
* total number of value comparison and position exchange determine the efficiency of a sorting algo.
1) Bubble sort
def bubbleSort(alist):
    for passnum in range(len(alist)-1,0,-1):
        for i in range(passnum):
            if alist[i]>alist[i+1]:
                alist[i],alist[i+1] = alist[i+1],alist[i]
    return alist

2) Selection sort:
* Compare to bubble sort, selection sort reduce the exchange effort, each round, exchange only happen ONCE
* Each round, only the largest element is put to the last position of the list
def selectionSort(my_list):
    for index in range(len(my_list)-1,0,-1):
        largest = 0
        print index
        for idx in range(1,index+1):
            if my_list[idx]>my_list[largest]:
                largest = idx
        my_list[largest],my_list[index] = my_list[index],my_list[largest]
    return my_list
    
3) Insertion sort:
* O(n) = n**2
def insertionSort(my_list):
    for item in range(1,len(my_list)):
        for i in range(item-1,-1,-1):
            if my_list[i] > my_list[item]:
                my_list[i],my_list[item] = my_list[item],my_list[i]
                item = i
    return my_list



Statistics:
1. Permutation:
1) In general, n distinct objects can be arranged in n! ways.
2) The number of permutations of n distinct objects taken r at a time, Pnr=n*(n-1)*(n-2)...*(n-(r-1))
3) The number of different permutations of n objects of which n1 are of one kind, n2 are of a second kind, ... nk are of a k-th kind is n!/(n1!*n2!*...nk!)
4) There are (n−1)! ways to arrange n distinct objects in a circle (where the clockwise and anti-clockwise arrangements are regarded as distinct.)
w
2. Combination:
1) Cnr=Pnr/r!
2) Cn0=Cnn=1
3) Cnr=Cn(n-r)

The Rule of Product says:
If there are n ways to perform action 1 and then by m ways to perform action
2, then there are n · m ways to perform action 1 followed by action 2.
*We will also call this the multiplication rule.

3. Probability terms:
1)Experiment: This is any process of observation or procedure that:

-Can be repeated (theoretically) an infinite number of times; and

-Has a well-defined set of possible outcomes.

2)Sample space: This is the set of all possible outcomes of an experiment.

3)Event: This is a subset of the sample space of an experiment.

4) The probability of an event E is the sum of the probabilities of all the outcomes in E.

4. Common probability problems:
1) Lottery odds:
http://www.intmath.com/counting-probability/singapore-toto.php
2) 

5. Conditional probability:
P(A|B): If A and B are two events, the probability that A occurs given that B has occurred
P(A|B)<student is French if he's female> = P(A⋂B)<student is French and female>/P(B)<student is female>
*Law of Total Probability:
Suppose the sample space Ω is divided into 3 disjoint events B1, B2, B3 (see the figure
below). Then for any event A:
P(A) = P(A ∩ B1) + P(A ∩ B2) + P(A ∩ B3)
 

6. Independent and dependent probability:
1) Indepedent:
P(A⋂B) = P(A)*P(B)
A is independent of B if P(A|B) = P(A)
2) Dependent:
P(A⋂B) = P(A)*P(B|A) = P(A)*P(B⋂A)/P(A)
* mutually exclusive events are NEVER independent, if A and B are mutually exclusive, P(A⋂B) = 0

7. non mutually exclusive events probability:
P(A or B) = P(A)+P(B)-P(A⋂B)

8. Bayes' Theorom:
If E1 and E2 are mutually exclusive:
P(E1∣E)=P(E1⋂E)/[P(E2⋂E)+P(E1⋂E)]

9. If use conditional probability or P(A or B):
See if the given probability is describing seperate events (there are steps), if so, conditional probability, if not, like defective check, it's P(A or B)

10. Steps of solve a problem:
1) define all events
2) determine the relations between events(independent events<IE>/dependent events<DE>/ME/non-ME)
3) use appropriate formula:
IE:
P(A⋂B) = P(A)*P(B)
P(A or B) = P(A)+P(B)-P(A⋂B)

DE:
P(A⋂B) = P(A)*P(B|A) = P(A)*P(B⋂A)/P(A)
P(A or B) = P(A)+P(B)-P(A⋂B)

ME:
P(A⋂B) = 0
P(A or B) = P(A)+P(B)

non_ME:
P(A⋂B) = P(A)*P(B|A) = P(A)*P(B⋂A)/P(A)
P(A or B) = P(A)+P(B)-P(A⋂B)

11. Probability Distributions:
1) Random Variables
P(X) or P(x1), P(x2)...
2) Discrete probability distribution:


3)Probability density function:
* the area under the bell curve between two variables, The area under each curve is \displaystyle{1}1
∫abf(x)dx=P(a≤X≤b)

4)Probabilities As Relative Frequency
* If an experiment is performed a sufficient number of times, then in the long run, the relative frequency of an event is called the probability of that event occurring.

5) Expected Value:
* X represent a discrete random variable
E(X) = μ = Σ (xi × P(xi))
* is also called the mean of the probability distribution
* we "expect" each of the possible values to occur with the given frequencies.
* it is a weighted average of the possible values, here probability is the weight
* Bernoulli(p) random variable. Find E(X): E(X) = p x 1 + (1 − p) x 0 = p
* If X and Y are random variables on a sample space Ω then: E(X + Y) = E(X) + E(Y);E(aX + b) = aE(X) + b
Roll two dice and let X be the sum. Find E(X):
Let X1 be the value on the first die and let X2 be the value on the second die. Since X = X1 + X2 we have E(X) = E(X1) + E(X2). Earlier we computed that E(X1) = E(X2) = 3.5, therefore E(X) = 7.
* For infinite random variables the mean does not always exist
 

4) Variance of a Random Variable
V(X) = σ2 = Σ[{X − E(X)}**2 × P(X) ]
Properties of variance:
1. If X and Y are independent then Var(X + Y ) = Var(X) + Var(Y )
2. For constants a and b, Var(aX + b) = a2Var(X)
3. Var(X) = E(X2) − E(X)2

5) Standard Deviation of the Probability Distribution
σ=√V(X)
* describes the spread of the distribution. Small standard deviation means small spread, large standard deviation means large spread

12. The Binomial Probability Distribution
Binomial variable: only two kinds of outcome: true vs. false; yes vs. no, etc
1) the probability of successes in n binomial trials:
http://www.intmath.com/counting-probability/12-binomial-probability-distributions.php
* Bernoulli distribution: flip coin for once

2) Mean and Variance of Binomial Distribution:
E(X) = μ = np
V(X) = σ2 = npq

3) Binomial probability distribution is always finite. You can only have a finite number of x.

4) Polling /conversion rate problem:
https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading6b.pdf
To run the poll a pollster selects n people at random and asks ’Do you support candidate
A or candidate B. Thus we can view the poll as a **sequence of n independent Bernoulli(p0)**
trials, X1, X2, . . . , Xn, where Xi is 1 if the i person prefers A and 0 if they prefer B. The
fraction of people polled that prefer A is just the average X.
We know that each Xi ∼ Bernoulli(p0) so,
E(Xi) = p0 and σXi = √p0(1 − p0).
Therefore, the central limit theorem tells us that
X ≈ N(p0, σ/√n), where σ = √p0(1 − p0).

13. The Poisson Probability Distribution
http://www.intmath.com/counting-probability/13-poisson-probability-distribution.php
Time of success in two disjoint time(or distance, area, volume, etc) intervals are independent
- car accident
- death rate in army due to horse kicking
- birth defect or mutation

E(X) = μ
V(X) = μ

13.5 Geometric Distributions
A geometric distribution models the number of tails before the first head in a sequence of
coin flips (Bernoulli trials).
P(X) = [(1-p)**k]P
Here k is the number of tails before the first head, and the probability of having a head in a flip is p

14. Exponential distribution: It models the waiting time for a continuous process to change state
P(X) = λ e**(−λx)
* Exponential distribution is memory less:
Suppose that the probability that a taxi arrives
within the first five minutes is p. If I wait five minutes and in fact no taxi arrives, then the
probability that a taxi arrives within the next five minutes is still p.

By contrast, suppose I were to instead go to Kendall Square subway station and wait for
the next inbound train. Since the trains are coordinated to follow a schedule (e.g., roughly
12 minutes between trains), if I wait five minutes without seeing a train then there is a far
greater probability that a train will arrive in the next five minutes. In particular, waiting
time for the subway is not memoryless, and a better model would be the uniform distribution
on the range [0,12].

* The exponential distribution is the precisely the continuous counterpart of the geometric distribution, which models the waiting time for a discrete process to change state.

15. Uniform Distribution (continuous and discrete:http://www.statisticshowto.com/uniform-distribution/):
The uniform distribution models any situation where all the outcomes are equally likely. Examples: fair coins (N = 2), dice (N = 6), birthdays (N = 365)...
* Uniform distribution is a probability distribution that has constant probability

P(X) = 1/(b-a) if x is in [a,b], otherwise, P(X) = 0

16. Normal distribution:
1) How to calculate probability of normal distribution:
The probability of P(x < a) is: Φ(a)
The probability of P(x > a) is: 1 – Φ(a)
The probability of P(a < Z < b) is: Φ(b) – Φ(a)


2) empirical/three-sigma rule:
for a normal distribution:
68% of the data will fall within one standard deviation of the mean.
95% of the data will fall within two standard deviations of the mean.
99.7% of the data will fall within three standard deviations of the mean

17. Descriptive Statistics and inferential statistics
1) Descriptive Statistics
IQR= Q3-Q1
mode is the value of the number which appears the most frequent.
*In statistics, a central tendency (or measure of central tendency) is a central or typical value for a probability distribution. It may also be called a center or location of the distribution. 

2) inferential statistics:
- confidence interval: 
*A Confidence Interval is a range of values we are fairly sure our true value lies in.
http://onlinelibrary.wiley.com/store/10.1002/9781119176459.app2/asset/app2.pdf;jsessionid=4A9786853B952C07AE11551CE4D33D64.f04t01?v=1&t=j5kdmt2y&s=adbcb36e33dea8411251d866fbef39d1f541b2b7
for 95% confidence interval:
X  ±  (Z*s/√(n)):
X is the mean
Z is 1.96for 95%
s is the standard deviation
n is the number of samples

or
p ± (Z*p(1-p)√(n))
p is the conversion rate, others mean the same as above


- conversion rate range
- statistical significance of an experiment: the likelihood of the difference of conversion rate between baseline and variation is not due to random chance
- To determine the observed difference in a statistical significance test, you will want to pay attention to two outputs: p-value and confidence interval around effect size.
- P-value: refers to the probability value of observing an effect from a sample. A p-value of < 0.05 is the conventional threshold for declaring statistical significance.
- Confidence interval around effect size: refers to the upper and lower bounds of what can happen with your experiment
- A statistically significant result isn’t attributed to chance and depends on two key variables: sample size and effect size.
- Sample size: refers to how large the sample for your experiment is.
- Effect size: refers to the size of the difference in results between two sample sets and indicates practical significance. If there is a small effect size (say a 0.1% increase in conversion rate) you will need a very large sample size to determine whether that difference is significant or just due to chance. However, if you observe a very large effect on your numbers, you will be able to validate it with a smaller sample size to a higher degree of confidence.
- Z score: how many standard diviations is a value away from the mean

3) General hypothesis testing
- The null hypothesis is essentially the "devil's advocate" position. That is, it assumes that whatever you are trying to prove did not happen (hint: it usually states that something equals zero). For example, the two different teaching methods did not result in different exam performances (i.e., zero difference).
The alternative hypothesis states the opposite and is usually the hypothesis you are trying to prove (e.g., the two different teaching methods did result in different exam performances). 
- significance level:
The level of statistical significance is often expressed as the so-called p-value
- how it works:
significance level/P value evaluates how well the sample data support the devil’s advocate argument that the null hypothesis is true. It measures how compatible your data are with the null hypothesis.

For example, suppose that a vaccine study produced a P value of 0.04. This P value indicates that if the vaccine had no effect, you’d obtain the observed difference or more in 4% of studies due to random sampling.

P values are calculated based on the assumptions that the null is true for the population and that the difference in the sample is caused entirely by random chance. Consequently, P values can’t tell you the probability that the null is true or false because it is 100% true from the perspective of the calculations.

https://www.khanacademy.org/math/statistics-probability/sampling-distributions-library/sample-means/v/sampling-distribution-of-the-sample-mean

https://www.khanacademy.org/math/statistics-probability/significance-tests-one-sample/tests-about-population-mean/v/hypothesis-testing-and-p-values

*** When plotting for hypothesis testing, that one big normal distribution looking curve is the "Sample distribution of sample mean", not the sample data itself! So each data point in that curve represent a sample and its mean. Hypothesis testing is built based on the central limit theorem. So it makes sense to conclude that if the alternative hypothesis mean is far away from the null hypothesis mean (z-score is large), the sample data from that alternative hypothesis probably does not follow the distribution of the null hypothesis data. Therefore we can prove that the alternative hypothesis generates new distribution of data.

- Type of errors:
In statistical hypothesis testing, a type I error is the incorrect rejection of a true null hypothesis (a "false positive"), while a type II error is incorrectly retaining a false null hypothesis (a "false negative").

- t-test:
*When sample size<30, the sampling distribution of mean follows t-distribution, and instead of using z-table, we use t-table to find the t-values. t-distribution is defined by the "degree of freedom(DF)". If the sample size=21, then DF=20. The bigger DF is, the "skinnier" the t-distribution curve is, and the thinner the curve tail is, therefore the probability density decreases when DF increase.
* In general, when the sample size<30, we'll assume that the original population is normally distributed.
* only consider one/two tails for t-distribution when checking the t-table, and both situations here are talking about symetrical

- One sample and two sample tests:
http://facweb.cs.depaul.edu/sjost/csc423/documents/test-descriptions.htm

19. Relationship between 2 quantitative variables:
y = a+bx
b = Slope = (N∑XY - (∑X)(∑Y)) / (N∑X**2 - (∑X)**2)
a = mean(Y)-bmean(X)

20. Modeling:
1) Linear Regression:
Predict a dependent continuous vale
* The standard error of a correlation coefficient is used to determine the confidence intervals around a true correlation of zero. If your correlation coefficient falls outside of this range, then it is significantly different than zero.
* Use the standard error of the coefficient to measure the precision of the estimate of the coefficient. The smaller the standard error, the more precise the estimate. Dividing the coefficient by its standard error calculates a t-value. If the p-value associated with this t-statistic is less than your alpha level, you conclude that the coefficient is significantly different from zero.

2) Logistic Regression:
Predict the probability of a value, bounded between (0,1)
* Changing the $\beta_0$ value shifts the curve horizontally, whereas changing the $\beta_1$ value changes the slope of the curve.
* Positive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability).
* logistic regression finds linear correlation between the log-odds and x, so log(p/1-p)=b+ax, here odds=p/1-p

3) Cross Validation (CV):
- CV is used to select the best model to train. Here, "two different models" would be: linear regression vs neural network. Once we have used cross-validation to select the better performing model, we train that model (whether it be the linear regression or the neural network) on all the data. 
- k-fold CV: k iterates over the sample data using a proportion for training and CV
- After k-fold cross validation, we’ll get k different model estimation errors (e1, e2 …..ek)(MSE for quantitative responses, misclassification rate for qualtative responses). Then we take standard deviation of all errors. Lower value of standard deviation suggests our model does not vary a lot with different subset of training data.
4) Over-fitting:
Over-fitting is normally caused by high model complexity over the training data.
5) Bias–variance tradeoff:
- Error due to Bias: The error due to bias is taken as the difference between the expected (or average) prediction of our model and the correct value which we are trying to predict.
high bias = underfitting
- Error due to Variance: The error due to variance is taken as the variability of a model prediction for a given data point.(the variance from k-fold CV for example)
high variance = overfitting
6)Interpret coefficient:
Interpreting the "temp" coefficient ($\beta_1$):
It is the change in y divided by change in x, or the "slope".
Thus, a temperature increase of 1 degree Celsius is associated with a rental increase of 9.17 bikes.
This is not a statement of causation.
$\beta_1$ would be negative if an increase in temperature was associated with a decrease in rentals.

7)Ways to measure accuracy of linear regression model:
Mathematically, if θ̂is an estimator forθ, then
- MBE(Bias Error): Bias[θ̂]=E[θ̂ −θ],
- Variance Error: Var[θ̂]=E[(θ̂ −E[θ̂])**2]
(Irreducible Error)
- MSE: mean of error square(variance). MSE[θ̂]=E[(θ̂ −θ)**2]. MSE = Var[θ̂]+(Bias[θ̂])**2. If bias = 0, then Var = MSE
- MAE: mean of error. MAE[θ̂]=E[|(θ̂ −θ)|]
- Cross Validation: evaluating the variance/different models


21. Sensitivity and Specificity:
1) Ways to measure accuracy of logistic regression model:
ROC:
- True positive rate (or sensitivity): TPR=TP/(TP+FN)
- False positive rate: FPR=FP/(FP+TN)FPR=FP/(FP+TN)
- True negative rate (or specificity): TNR=TN/(FP+TN)
* for each (probability/classification)threshold, there will be a pair: (sensitivity,specificity)
https://stats.stackexchange.com/questions/61829/relation-between-true-positive-false-positive-false-negative-and-true-negative
- ROC curve plots plotting "sensitivity" against "1-specificity" using all possible predicting threshold(range from 0 to 1)
- ROC helps visualise the impact of a chosen threshold(depends on if the goal is to maximize TP or minimize the FP)
- For different modeling, the goal is to minimize the AUC(area under curve)
- FP: Type I error; FN: Type II error
- AUC: area under curve
* the AUC of a classifier is equal to the probability that the classifier will rank(by the probability of being positive,what the logistic regression model is predicting) a randomly chosen positive example higher than a randomly chosen negative example. Therefore, the bigger the AUC is, the better the model performs

Misclassification rate:
- error rate for only a single threshold
 
 
Calculus:
1. Definite Integral:
1) Reverse power rule:
the antiderivative of f(x) is F(x) is: for example: Sx**2dx: here the power is 2, so the antiderivative(integral) should have a +1 power, which is 2+1=3, and also we need to devide the whole f(x) with the new power(3 in this case), so Sx**2dx=x**3/3
2) Evaluate definite integral:
https://www.khanacademy.org/math/integral-calculus/definite-integral-evaluation-ic
